{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Back to Basics: CNNs",
      "provenance": [],
      "authorship_tag": "ABX9TyOxC36vihXBkcE33mSHzioB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Otobi1/Back-to-Basics-A-Refresher-/blob/master/Back_to_Basics_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSEYD28OfPkb"
      },
      "source": [
        "## Set up \r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import torch \r\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "759r6mk1ZUM_"
      },
      "source": [
        "SEED = 1234"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MqZJHqAZV1x"
      },
      "source": [
        "def set_seeds(seed = 1234):\r\n",
        "  \"\"\"Set seed for reproducibility.\"\"\"\r\n",
        "  np.random.seed(seed)\r\n",
        "  random.seed(seed)\r\n",
        "  torch.manual_seed(seed)\r\n",
        "  torch.cuda.manual_seed(seed)\r\n",
        "  torch.cuda.manual_seed_all(seed) # multi GPU"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP0JoHAEZ2XM"
      },
      "source": [
        "# Set seed for reproducibility \r\n",
        "\r\n",
        "set_seeds(seed=SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWPpxlKIawys",
        "outputId": "810ed78b-0c6e-4961-e473-3ad3e40e0be1"
      },
      "source": [
        "# Set device \r\n",
        "\r\n",
        "cuda = True\r\n",
        "device = torch.device(\"cuda\" if (\r\n",
        "    torch.cuda.is_available() and cuda) else \"cpu\")\r\n",
        "torch.set_default_tensor_type(\"torch.FloatTensor\")\r\n",
        "if device.type == \"cuda\":\r\n",
        "  torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\r\n",
        "print (device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "hNTtW2TjbX90",
        "outputId": "a735b59c-cfcc-4bf7-e06c-90f7d7007be5"
      },
      "source": [
        "# Load data \r\n",
        "# - corpus of news article from http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html \r\n",
        "\r\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/madewithml/main/datasets/news.csv\"\r\n",
        "df = pd.read_csv(url, header = 0) # load\r\n",
        "df = df.sample(frac = 1).reset_index(drop = True) # shuffle\r\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The New Faces of Reality TV</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category\n",
              "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
              "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
              "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
              "3          Growing Signs of a Slowing on Wall Street  Business\n",
              "4                        The New Faces of Reality TV     World"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlO6lmtb5rN"
      },
      "source": [
        "# Preprocessing \r\n",
        "# - to clean up the data, convert to lower text, remove filler words and filter using regex.\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "import re"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDTD_WxOcgyF",
        "outputId": "68a646c1-cfb3-421c-a24a-7c7880b12fb5"
      },
      "source": [
        "nltk.download(\"stopwords\")\r\n",
        "STOPWORDS = stopwords.words(\"english\")\r\n",
        "print (STOPWORDS[:5])\r\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oH6OeZdQld"
      },
      "source": [
        "def preprocess(text, stopwords = STOPWORDS):\r\n",
        "  \"\"\"Conditional preprocessing on our text unique to the task.\"\"\"\r\n",
        "  # Lower \r\n",
        "  text = text.lower()\r\n",
        "\r\n",
        "  # Remove stopwords\r\n",
        "  pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\r\n",
        "  text = pattern.sub(\" \", text)\r\n",
        "\r\n",
        "  # Remove words in parenthesis\r\n",
        "  text = re.sub(r\"\\([^)]*\\)\", \" \", text)\r\n",
        "\r\n",
        "  # Spacing and filters \r\n",
        "  text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\r\n",
        "  text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric characters\r\n",
        "  text = re.sub(' +', ' ', text) # remove multiple spaces \r\n",
        "  text = text.strip()\r\n",
        "\r\n",
        "  return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K-DBYnbtgJXf",
        "outputId": "7bd1e308-871a-4514-b36e-5ec34151d77e"
      },
      "source": [
        "# Sample \r\n",
        "\r\n",
        "text = \"Great week for the NYSE\"\r\n",
        "preprocess(text = text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'great week nyse'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbVH3XrNgmJk",
        "outputId": "dde3b0d7-4e20-4846-8e38-24e26be283a1"
      },
      "source": [
        "# Apply to dataframe\r\n",
        "\r\n",
        "preprocessed_df = df.copy()\r\n",
        "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\r\n",
        "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
            "\n",
            "sharon accepts plan reduce gaza army operation haaretz says\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TLGqCnLhMJL"
      },
      "source": [
        "# if you have preprocessing steps like standardisation, that are calculated, you need to separate the training an dtest set first before applying those operations. \r\n",
        "# this is because we cannot apply any knowledge gained from the test set accidentally (data leak during preprocessing/training). \r\n",
        "# - for global preprocessing steps like the functin above, where we arent learning anything from the data itself, we can perform them before splitting the data."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvGalMXshdP5"
      },
      "source": [
        "# Split the data \r\n",
        "\r\n",
        "import collections\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2_5ayPKh7ak"
      },
      "source": [
        "TRAIN_SIZE = 0.7\r\n",
        "VAL_SIZE = 0.15\r\n",
        "TEST_SIZE = 0.15"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToTVRdVRiu6m"
      },
      "source": [
        "def train_val_test_split(X, y, train_size):\r\n",
        "  \"\"\"Split dataset into data split.\"\"\"\r\n",
        "  X_train, X_, y_train, y_ = train_test_split(X, y, train_size = TRAIN_SIZE, stratify = y)\r\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size = 0.5, stratify = y_)\r\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4dHEIRQjZoX"
      },
      "source": [
        "# Data\r\n",
        "X = preprocessed_df[\"title\"].values \r\n",
        "y = preprocessed_df[\"category\"].values"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJxEUiLbjh4V",
        "outputId": "001ebc8a-542a-4dd8-9555-e62d8c2c1c0f"
      },
      "source": [
        "# Create data splits\r\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\r\n",
        "    X = X, y = y, train_size = TRAIN_SIZE)\r\n",
        "\r\n",
        "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\r\n",
        "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\r\n",
        "print (f\"X_train: {X_test.shape}, y_test: {y_test.shape}\")\r\n",
        "print (f\"Sample point: {X_train[0]} -> {y_train[0]}\")\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (84000,), y_train: (84000,)\n",
            "X_val: (18000,), y_val: (18000,)\n",
            "X_train: (18000,), y_test: (18000,)\n",
            "Sample point: china battles north korea nuclear talks -> World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtvU0BHUkSEr"
      },
      "source": [
        "# Label Encoding \r\n",
        "# to encode the text labels into unique indices\r\n",
        "\r\n",
        "import itertools"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2cewudKkgVY"
      },
      "source": [
        "class LabelEncoder(object):\r\n",
        "  \"\"\"Label encoder for tag labels.\"\"\"\r\n",
        "  def __init__(self, class_to_index = {}):\r\n",
        "    self.class_to_index = class_to_index\r\n",
        "    self.index_to_class = {v: k for k, v in self.class_to_index.items()}\r\n",
        "    self.classes = list(self.class_to_index.keys())\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.class_to_index)\r\n",
        "\r\n",
        "  def __str__(self):\r\n",
        "    return f\"<LabelEncoder(num_classes = {len(self)})>\"\r\n",
        "  \r\n",
        "  def fit(self, y):\r\n",
        "    classes = np.unique(y_train)\r\n",
        "    for i, class_ in enumerate(classes):\r\n",
        "      self.class_to_index[class_] = i\r\n",
        "    self.index_to_class = {v: k for k, v in self.class_to_index.items()}\r\n",
        "    self.classes = list(self.class_to_index.keys())\r\n",
        "    return self\r\n",
        "\r\n",
        "  def encode(self, y):\r\n",
        "    encoded = np.zeros((len(y)), dtype = int)\r\n",
        "    for i, item in enumerate(y):\r\n",
        "      encoded[i] = self.class_to_index[item]\r\n",
        "    return encoded\r\n",
        "\r\n",
        "  def decode(self, y):\r\n",
        "    classes = []\r\n",
        "    for i, item in enumerate(y):\r\n",
        "      classes.append(self.index_to_class[item])\r\n",
        "    return classes\r\n",
        "\r\n",
        "  def save(self, fp):\r\n",
        "    with open(fp, \"w\") as fp:\r\n",
        "      contents = {\"class_to_index\": self.class_to_index}\r\n",
        "      json.dump(contents, fp, indent = 4, sort_keys = False)\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def load(cls, fp):\r\n",
        "    with open(fp, \"r\") as fp:\r\n",
        "      kwargs = json.load(fp = fp)\r\n",
        "    return clas(**kwargs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwntyBauo_QA",
        "outputId": "797ac62f-3cb0-40a9-ba1d-72b8e7c8b01d"
      },
      "source": [
        "# Encode \r\n",
        "\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "label_encoder.fit(y_train)\r\n",
        "NUM_CLASSES = len(label_encoder)\r\n",
        "label_encoder.class_to_index"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHY1TClrpXP6",
        "outputId": "40579998-b13d-47c9-d8af-9534b5fc5e28"
      },
      "source": [
        "# Converting labels to tokens \r\n",
        "\r\n",
        "print (f\"y_train[0]: {y_train[0]}\")\r\n",
        "\r\n",
        "y_train = label_encoder.encode(y_train)\r\n",
        "y_val = label_encoder.encode(y_val)\r\n",
        "y_test = label_encoder.encode(y_test)\r\n",
        "\r\n",
        "print (f\"y_train[0]: {y_train[0]}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train[0]: World\n",
            "y_train[0]: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFHTuNzTp3sq",
        "outputId": "a2035b55-792f-452d-8e84-30f49fa557c3"
      },
      "source": [
        "# Class weights \r\n",
        "\r\n",
        "counts = np.bincount(y_train)\r\n",
        "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\r\n",
        "print (f\"counts: {counts}\\nweights: {class_weights}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "counts: [21000 21000 21000 21000]\n",
            "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYDrlCrFrD_3"
      },
      "source": [
        "# Tokenizer \r\n",
        "\r\n",
        "import json\r\n",
        "from collections import Counter\r\n",
        "from more_itertools import take"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFX9mkPwriDZ"
      },
      "source": [
        "class Tokeniser(object):\r\n",
        "  def __init__(self, char_level, num_tokens = None,\r\n",
        "               pad_token = \"<PAD>\", oov_token = \"<UNK>\", \r\n",
        "               token_to_index = None):\r\n",
        "    self.char_level = char_level\r\n",
        "    self.separator = \" \" if self.char_level else \" \"\r\n",
        "    if num_tokens: num_tokens -= 2 # pad + unk tokens\r\n",
        "    self.num_tokens = num_tokens\r\n",
        "    self.oov_token = oov_token\r\n",
        "    if not token_to_index:\r\n",
        "      token_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\r\n",
        "    self.token_to_index = token_to_index\r\n",
        "    self.index_to_token = {v: k for k, v in self.token_to_index.items()}\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.token_to_index)\r\n",
        "\r\n",
        "  def __str__(self):\r\n",
        "    return f\"<Tokeniser(num_tokens = {len(self)})>\"\r\n",
        "\r\n",
        "  def fit_on_texts(self, texts):\r\n",
        "    if self.char_level:\r\n",
        "      all_tokens = [token for text in texts for token in text]\r\n",
        "    if not self.char_level:\r\n",
        "      all_tokens = [token for text in texts for token in text.split(\" \")]\r\n",
        "    counts = Counter(all_tokens).most_common(self.num_tokens)\r\n",
        "    self.min_token_freq = counts[-1][1]\r\n",
        "    for token, count in counts:\r\n",
        "      index = len(self)\r\n",
        "      self.token_to_index[token] = index\r\n",
        "      self.index_to_token[index] = token\r\n",
        "    return self\r\n",
        "\r\n",
        "  def texts_to_sequence(self, texts):\r\n",
        "    sequences = []\r\n",
        "    for text in texts:\r\n",
        "      if not self.char_level:\r\n",
        "        text = text.split(' ')\r\n",
        "      sequence = []\r\n",
        "      for token in text: \r\n",
        "        sequence.append(self.token_to_index.get(\r\n",
        "            token, self.token_to_index[self.oov_token]))\r\n",
        "      sequences.append(np.asarray(sequence))\r\n",
        "    return sequences\r\n",
        "  \r\n",
        "  def sequences_to_texts(self, sequences):\r\n",
        "    texts = []\r\n",
        "    for sequence in sequences:\r\n",
        "      text = []\r\n",
        "      for index in sequence :\r\n",
        "        text.append(self.index_to_token.get(index, self.oov_token))\r\n",
        "      texts.append(self.separator.join([token for token in text]))\r\n",
        "    return texts\r\n",
        "  \r\n",
        "  def save(self, fp):\r\n",
        "    with open(fp, \"w\") as fp:\r\n",
        "      contents = {\r\n",
        "          \"char_level\": self.char_level, \r\n",
        "          \"oov_token\": self.oov_token, \r\n",
        "          \"token_to_index\": self.token_to_index\r\n",
        "      }\r\n",
        "      json.dump(contents, fp, indent = 4, sort_keys = False)\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def load(cls, fp):\r\n",
        "    with open(fp, \"r\") as fp:\r\n",
        "      kwargs = json.load(fp = fp)\r\n",
        "    return cls(**kwargs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTZBh3bAwQUL"
      },
      "source": [
        "## - we will restrict the number of tokens in our tokenizer to the top 500 most frequent tokens (stop words already removed)\r\n",
        "# -- because the full vocabulary (approx 30k) is too large to run on google colab\r\n",
        "\r\n",
        "# ** it is important that we are only using the training data split because during inference, the model will not always know every token\r\n",
        "# -- so it is important to replicate that scenario with the validation and test split. "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSQgaU3sxiWJ",
        "outputId": "e50cfd02-6313-42b6-8469-4a5db7c038a2"
      },
      "source": [
        "# Tokenise\r\n",
        "\r\n",
        "tokeniser = Tokeniser(char_level = False, num_tokens = 500)\r\n",
        "tokeniser.fit_on_texts(texts = X_train)\r\n",
        "VOCAB_SIZE = len(tokeniser)\r\n",
        "\r\n",
        "print (tokeniser)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Tokeniser(num_tokens = 500)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCShuyiwyXQc",
        "outputId": "c857b551-ddad-406f-9fb4-9792e6908a51"
      },
      "source": [
        "# Sample of tokens \r\n",
        "\r\n",
        "print (take(5, tokeniser.token_to_index.items()))\r\n",
        "print (f\"least freq tokens freq: {tokeniser.min_token_freq}\") # use this to adjust num tokens"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
            "least freq tokens freq: 166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y1tmtwozS5O",
        "outputId": "0906f115-09b2-403c-d9ad-f7a099032904"
      },
      "source": [
        "# Convert texts to sequences of indices \r\n",
        "\r\n",
        "X_train = tokeniser.texts_to_sequence(X_train)\r\n",
        "X_val = tokeniser.texts_to_sequence(X_val)\r\n",
        "X_test = tokeniser.texts_to_sequence(X_test)\r\n",
        "\r\n",
        "preprocessed_text = tokeniser.sequences_to_texts([X_train[0]])[0]\r\n",
        "print (\"Text to indices: \\n\"\r\n",
        "    f\" (preprocessed) -> {preprocessed_text}\\n\"\r\n",
        "    f\" (tokenised) -> {X_train[0]}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to indices: \n",
            " (preprocessed) -> china <UNK> north korea nuclear talks\n",
            " (tokenised) -> [ 16   1 285 142 114  24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXswfqRo0GmE"
      },
      "source": [
        "# One-hot Encoding \r\n",
        "\r\n",
        "# - Creates a binary column fro each unique value of each feature. \r\n",
        "# -- All the values for the token will be 0 except the index of that specific token"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG7EDYm513fS"
      },
      "source": [
        "def to_categorical(seq, num_classes):\r\n",
        "  \"\"\"One-hot encode a sequence of tokens.\"\"\"\r\n",
        "  one_hot = np.zeros((len(seq), num_classes))\r\n",
        "  for i, item in enumerate(seq):\r\n",
        "    one_hot[i, item] = 1.\r\n",
        "  return one_hot"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuH7iXsj2Ww1",
        "outputId": "79464525-8991-44dd-f80a-16a23b610ae7"
      },
      "source": [
        "# One-hot encoding \r\n",
        "print (X_train[0])\r\n",
        "print (len(X_train[0]))\r\n",
        "cat = to_categorical(seq = X_train[0], num_classes = len(tokeniser))\r\n",
        "\r\n",
        "print (cat)\r\n",
        "print (cat.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 16   1 285 142 114  24]\n",
            "6\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(6, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bkcutvt2rQP"
      },
      "source": [
        "# Convert tokens to one-hot\r\n",
        "\r\n",
        "vocab_size = len(tokeniser)\r\n",
        "X_train = [to_categorical(seq, num_classes = vocab_size) for seq in X_train]\r\n",
        "X_val = [to_categorical(seq, num_classes = vocab_size) for seq in X_val]\r\n",
        "X_test = [to_categorical(seq, num_classes = vocab_size) for seq in X_test]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQQZ4P93Oy0"
      },
      "source": [
        "# Padding \r\n",
        "\r\n",
        "# - all the inputs have varying lengths, but each batch needs to e uniformly shaped\r\n",
        "# - we can use padding to make all the inputs in the batch the same length\r\n",
        "# - the padding index will be 0\r\n",
        "\r\n",
        "## ** one-hot encoding creates a batch of shape (N, max_seq_len, vocab_size) so we'll need to pad 3D sequences "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFz7Go4m30W3"
      },
      "source": [
        "def pad_sequences(sequences, max_seq_len = 0):\r\n",
        "  \"\"\"Pad sequences to max length in sequence.\"\"\"\r\n",
        "  max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\r\n",
        "  num_classes = sequences[0].shape[-1]\r\n",
        "  padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))\r\n",
        "  for i, sequence in enumerate(sequences):\r\n",
        "    padded_sequences[i][:len(sequence)] = sequence\r\n",
        "  return padded_sequences"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHFOCr1H4rE_",
        "outputId": "a50bb6b7-39f8-4d8d-dd81-2107800822fa"
      },
      "source": [
        "# 3D sequences \r\n",
        "\r\n",
        "print (X_train[0].shape, X_train[1].shape, X_train[2].shape)\r\n",
        "padded = pad_sequences(X_train[0:3])\r\n",
        "print (padded.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 500) (5, 500) (6, 500)\n",
            "(3, 6, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDFPyGyJ5CAT"
      },
      "source": [
        "# Dataset\r\n",
        "# - here we need to create datasets and dataloaders to be able to efficiently create batches with the data splits \r\n",
        "\r\n",
        "FILTER_SIZE = 1 # unigram"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTMezleJ6U7q"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, X, y, max_filter_size):\r\n",
        "    self.X = X\r\n",
        "    self.y = y\r\n",
        "    self.max_filter_size = max_filter_size\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.y)\r\n",
        "\r\n",
        "  def __str__(self):\r\n",
        "    return f\"<Dataset(N = {len(self)})>\"\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    X = self.X[index]\r\n",
        "    y = self.y[index]\r\n",
        "    return [X, y]\r\n",
        "\r\n",
        "  def collate_fn(self, batch):\r\n",
        "    \"\"\"Processing on batch.\"\"\"\r\n",
        "    # Get inputs\r\n",
        "    X = np.array(batch, dtype = object)[:, 0]\r\n",
        "    y = np.stack(np.array(batch, dtype = object)[:, 1], axis = 0)\r\n",
        "\r\n",
        "    # Pad sequences \r\n",
        "    X = pad_sequences(X, max_seq_len = self.max_filter_size)\r\n",
        "\r\n",
        "    # Cast\r\n",
        "    X = torch.FloatTensor(X.astype(np.int32))\r\n",
        "    y = torch.LongTensor(y.astype(np.int32))\r\n",
        "\r\n",
        "    return X, y\r\n",
        "\r\n",
        "  def create_dataloader(self, batch_size, shuffle = False, drop_last = False):\r\n",
        "    return torch.utils.data.DataLoader(\r\n",
        "        dataset = self, batch_size = batch_size, collate_fn = self.collate_fn, \r\n",
        "        shuffle = shuffle, drop_last = drop_last, pin_memory = True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXoY27PZ8Q5u",
        "outputId": "4675b4e6-7ec5-45c7-9ae2-a9606b094f06"
      },
      "source": [
        "# Create datasets for embedding \r\n",
        "\r\n",
        "train_dataset = Dataset(X = X_train, y = y_train, max_filter_size = FILTER_SIZE)\r\n",
        "val_dataset = Dataset(X = X_val, y = y_val, max_filter_size = FILTER_SIZE)\r\n",
        "test_dataset = Dataset(X = X_test, y = y_test, max_filter_size = FILTER_SIZE)\r\n",
        "print (\"Datasets: \\n\"\r\n",
        "    f\" Train dataset: {train_dataset.__str__()}\\n\"\r\n",
        "    f\" Val dataset: {val_dataset.__str__()}\\n\"\r\n",
        "    f\" Test dataset: {test_dataset.__str__()}\\n\"\r\n",
        "    \"Sample Point: \\n\"\r\n",
        "    f\" X: {test_dataset[0][0]}\\n\"\r\n",
        "    f\" y: {test_dataset[0][1]}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datasets: \n",
            " Train dataset: <Dataset(N = 84000)>\n",
            " Val dataset: <Dataset(N = 18000)>\n",
            " Test dataset: <Dataset(N = 18000)>\n",
            "Sample Point: \n",
            " X: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            " y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En480uM4-uS7",
        "outputId": "6c2661bd-c955-4f04-9395-4e14761d7073"
      },
      "source": [
        "# Create dataloaders \r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "train_dataloader = train_dataset.create_dataloader(batch_size = batch_size)\r\n",
        "val_dataloader = val_dataset.create_dataloader(batch_size = batch_size)\r\n",
        "test_dataloader = test_dataset.create_dataloader(batch_size = batch_size)\r\n",
        "batch_X, batch_y = next(iter(test_dataloader))\r\n",
        "print (\"Sample batch:\\n\"\r\n",
        "    f\" X: {list(batch_X.size())}\\n\"\r\n",
        "    f\" y: {list(batch_y.size())}\\n\"\r\n",
        "    \"Sample point:\\n\"\r\n",
        "    f\" X: {batch_X[0]}\\n\"\r\n",
        "    f\" y: {batch_y[0]}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample batch:\n",
            " X: [64, 14, 500]\n",
            " y: [64]\n",
            "Sample point:\n",
            " X: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cpu')\n",
            " y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3T_oqa4DcoL"
      },
      "source": [
        "# CNN - Convolutional Neural Networks \r\n",
        "\r\n",
        "# - Here we will learn about CNNs by applying them on 1D text data\r\n",
        "# In the example below, we have a batch of N samples where wach sample has 8 characters and each char represented by an array of 10 values (vocab size = 10)\r\n",
        "# - this gives our inputs the size (N, 8, 10)\r\n",
        "\r\n",
        "# -- with PyTorch, when dealing with convs, the inputs X need to have the channels as the second dimension, so our inputs will be (N,10,8)\r\n",
        "\r\n",
        "import math\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfdX2dWkG4KD",
        "outputId": "fc2e51c6-62a0-4076-fd90-8f8a99008775"
      },
      "source": [
        "# Assume all our inputs are padded to have the same words\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "max_seq_len = 8 # words per input\r\n",
        "vocab_size = 10 # one hot size\r\n",
        "x = torch.randn(batch_size, max_seq_len, vocab_size)\r\n",
        "print (f\" X:{x.shape}\")\r\n",
        "x = x.transpose(1, 2)\r\n",
        "print (f\" X:{x.shape}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " X:torch.Size([64, 8, 10])\n",
            " X:torch.Size([64, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flM_mMvwH80y"
      },
      "source": [
        "# At the core of CNNs are filters, also known as weights, kernels etc), which convolve (slide) across our input to extract relevant features. \r\n",
        "# the filters are initialised randomly but learn to act as feature extractors via parameter sharing.\r\n",
        "\r\n",
        "# We will use a conv1d layer to process our inouts "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGCgRgglLTMD",
        "outputId": "765ac4c8-6d35-4b11-f3c4-9e935042dec8"
      },
      "source": [
        "# Convolutional filters (Valid padding)\r\n",
        "\r\n",
        "vocab_size = 10 # one-hot size\r\n",
        "num_filters = 50 # num of filters \r\n",
        "filter_size = 3 # filters are 3 X 3 \r\n",
        "stride = 1\r\n",
        "padding = 0 # valid padding (no padding)\r\n",
        "conv1 = nn.Conv1d(in_channels = vocab_size, out_channels = num_filters, \r\n",
        "                  kernel_size = filter_size, stride = stride, \r\n",
        "                  padding = padding, padding_mode = \"zeros\")\r\n",
        "print (\"conv: {}\".format(conv1.weight.shape))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv: torch.Size([50, 10, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBK_HURrMKJl",
        "outputId": "9ff284cc-e22e-4ba4-d4ba-b2dd5340b1ab"
      },
      "source": [
        "# Forward pass\r\n",
        "z = conv1(x)\r\n",
        "print (f\"z: {z.shape}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z: torch.Size([64, 50, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ3NIvVtMTjV"
      },
      "source": [
        "# Now, we'll add padding so that the convolutional outputs are the same shape as our inputs. \r\n",
        "# - we want our output to have the same width as our input. "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVMB5lgXNPVT",
        "outputId": "28522ec8-94b1-4864-a5c0-5ef057d121f3"
      },
      "source": [
        "# Convolutional filters (Same padding)\r\n",
        "\r\n",
        "vocab_size = 10 # one-hot size\r\n",
        "num_filters = 50 # num filters \r\n",
        "filter_size = 3 # filters are 3 X 3\r\n",
        "stride = 1\r\n",
        "conv = nn.Conv1d(in_channels = vocab_size, out_channels = num_filters, \r\n",
        "                 kernel_size = filter_size, stride = stride)\r\n",
        "print (\"conv: {}\".format(conv.weight.shape))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv: torch.Size([50, 10, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqdlRQTuN4LV",
        "outputId": "3d69551c-2f3c-403d-db6a-35b94e7298d1"
      },
      "source": [
        "# Same padding \r\n",
        "\r\n",
        "padding_left = int((conv.stride[0] * (max_seq_len-1) - max_seq_len + filter_size) / 2)\r\n",
        "padding_right = int(math.ceil((conv.stride[0] * (max_seq_len - 1) - max_seq_len + filter_size) / 2))\r\n",
        "print (f\"padding: {(padding_left, padding_right)}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padding: (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEGUkVCgdMmT",
        "outputId": "523b25e3-56a0-40a4-bc70-82407396eb56"
      },
      "source": [
        "# Forward pass \r\n",
        "\r\n",
        "z = conv(F.pad(x, (padding_left, padding_right)))\r\n",
        "print (f\"z: {z.shape}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z: torch.Size([64, 50, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOMcR8txdk-g",
        "outputId": "8e7dcd66-0106-4474-b594-168592b89a6f"
      },
      "source": [
        "# Pooling \r\n",
        "\r\n",
        "# - the result of the convolving filters on an input is a feature map. Due to the nature of convolution and overlaps,our feature map will have lots of redundant information.\r\n",
        "# - Pooling is a way to summarise a high-dimensional feature map into a lower dimensional one for simplified downstream computation. \r\n",
        "# -- the pooling operation can be the max value, average etc\r\n",
        "\r\n",
        "# Max pooling \r\n",
        "\r\n",
        "pool_output = F.max_pool1d(z, z.size(2))\r\n",
        "print (\"Size: {}\".format(pool_output.shape))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([64, 50, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZSujn3Efqiv"
      },
      "source": [
        "# Batch normalisation \r\n",
        "\r\n",
        "# - this is an operation that will standardise the activations from the previous layer \r\n",
        "# recall that we've previously standardised our inputs so that the model can optimise quickly with larger learning learning rates\r\n",
        "# - here we will use the same concept  but we will continue to maintain standardised values throughout the forward pass to further aid optimisation"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8TyhVdkR7N",
        "outputId": "693a1d41-ee64-4f3d-c80e-bae436c7bff8"
      },
      "source": [
        "batch_norm = nn.BatchNorm1d(num_features = num_filters)\r\n",
        "z = batch_norm(conv(x)) # applied to activations (after conv layer & before pooling)\r\n",
        "print (f\"z: {z.shape}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z: torch.Size([64, 50, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8bwjkmvkp8N",
        "outputId": "df0c4113-ddae-410c-a252-4491f4e926ef"
      },
      "source": [
        "# Mean and std before batchnorm\r\n",
        "print (f\"mean: {torch.mean(conv1(x)):.2f}, std: {torch.std(conv(x)):.2f}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: 0.01, std: 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVnRsWN4lCoa",
        "outputId": "dde371bf-92d8-425c-deb0-0a506a9d1984"
      },
      "source": [
        "# Mean and std after batchnorm\r\n",
        "print (f\"mean: {torch.mean(z):.2f}, std: {torch.std(z):.2f}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: 0.00, std: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dRV-VYzlSMQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}