{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Back to Basics: CNNs",
      "provenance": [],
      "authorship_tag": "ABX9TyPoJ5h6a1vL8117Ya8R+FHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Otobi1/Back-to-Basics-A-Refresher-/blob/master/Back_to_Basics_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSEYD28OfPkb"
      },
      "source": [
        "## Set up \r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import random \r\n",
        "import torch \r\n",
        "import torch.nn as nn"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "759r6mk1ZUM_"
      },
      "source": [
        "SEED = 1234"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MqZJHqAZV1x"
      },
      "source": [
        "def set_seeds(seed = 1234):\r\n",
        "  \"\"\"Set seed for reproducibility.\"\"\"\r\n",
        "  np.random.seed(seed)\r\n",
        "  random.seed(seed)\r\n",
        "  torch.manual_seed(seed)\r\n",
        "  torch.cuda.manual_seed(seed)\r\n",
        "  torch.cuda.manual_seed_all(seed) # multi GPU"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP0JoHAEZ2XM"
      },
      "source": [
        "# Set seed for reproducibility \r\n",
        "\r\n",
        "set_seeds(seed=SEED)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWPpxlKIawys",
        "outputId": "5732edac-d984-401f-b595-4f82618692eb"
      },
      "source": [
        "# Set device \r\n",
        "\r\n",
        "cuda = True\r\n",
        "device = torch.device(\"cuda\" if (\r\n",
        "    torch.cuda.is_available() and cuda) else \"cpu\")\r\n",
        "torch.set_default_tensor_type(\"torch.FloatTensor\")\r\n",
        "if device.type == \"cuda\":\r\n",
        "  torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\r\n",
        "print (device)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "hNTtW2TjbX90",
        "outputId": "a68b4e8d-4699-46fc-fd43-cafeec83519c"
      },
      "source": [
        "# Load data \r\n",
        "# - corpus of news article from http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html \r\n",
        "\r\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/madewithml/main/datasets/news.csv\"\r\n",
        "df = pd.read_csv(url, header = 0) # load\r\n",
        "df = df.sample(frac = 1).reset_index(drop = True) # shuffle\r\n",
        "df.head()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The New Faces of Reality TV</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category\n",
              "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
              "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
              "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
              "3          Growing Signs of a Slowing on Wall Street  Business\n",
              "4                        The New Faces of Reality TV     World"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlO6lmtb5rN"
      },
      "source": [
        "# Preprocessing \r\n",
        "# - to clean up the data, convert to lower text, remove filler words and filter using regex.\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "import re"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDTD_WxOcgyF",
        "outputId": "556c833e-4980-47bf-a7a8-c94dcfe0fa5b"
      },
      "source": [
        "nltk.download(\"stopwords\")\r\n",
        "STOPWORDS = stopwords.words(\"english\")\r\n",
        "print (STOPWORDS[:5])\r\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oH6OeZdQld"
      },
      "source": [
        "def preprocess(text, stopwords = STOPWORDS):\r\n",
        "  \"\"\"Conditional preprocessing on our text unique to the task.\"\"\"\r\n",
        "  # Lower \r\n",
        "  text = text.lower()\r\n",
        "\r\n",
        "  # Remove stopwords\r\n",
        "  pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\r\n",
        "  text = pattern.sub(\" \", text)\r\n",
        "\r\n",
        "  # Remove words in parenthesis\r\n",
        "  text = re.sub(r\"\\([^)]*\\)\", \" \", text)\r\n",
        "\r\n",
        "  # Spacing and filters \r\n",
        "  text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\r\n",
        "  text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric characters\r\n",
        "  text = re.sub(' +', ' ', text) # remove multiple spaces \r\n",
        "  text = text.strip()\r\n",
        "\r\n",
        "  return text"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K-DBYnbtgJXf",
        "outputId": "d5d0b514-0abe-45b5-ca0f-bb3828a0456b"
      },
      "source": [
        "# Sample \r\n",
        "\r\n",
        "text = \"Great week for the NYSE\"\r\n",
        "preprocess(text = text)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'great week nyse'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbVH3XrNgmJk",
        "outputId": "9e5e6946-fa36-4e65-b88b-bf26e94dacd4"
      },
      "source": [
        "# Apply to dataframe\r\n",
        "\r\n",
        "preprocessed_df = df.copy()\r\n",
        "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\r\n",
        "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
            "\n",
            "sharon accepts plan reduce gaza army operation haaretz says\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TLGqCnLhMJL"
      },
      "source": [
        "# if you have preprocessing steps like standardisation, that are calculated, you need to separate the training an dtest set first before applying those operations. \r\n",
        "# this is because we cannot apply any knowledge gained from the test set accidentally (data leak during preprocessing/training). \r\n",
        "# - for global preprocessing steps like the functin above, where we arent learning anything from the data itself, we can perform them before splitting the data."
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvGalMXshdP5"
      },
      "source": [
        "# Split the data \r\n",
        "\r\n",
        "import collections\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2_5ayPKh7ak"
      },
      "source": [
        "TRAIN_SIZE = 0.7\r\n",
        "VAL_SIZE = 0.15\r\n",
        "TEST_SIZE = 0.15"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToTVRdVRiu6m"
      },
      "source": [
        "def train_val_test_split(X, y, train_size):\r\n",
        "  \"\"\"Split dataset into data split.\"\"\"\r\n",
        "  X_train, X_, y_train, y_ = train_test_split(X, y, train_size = TRAIN_SIZE, stratify = y)\r\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size = 0.5, stratify = y_)\r\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4dHEIRQjZoX"
      },
      "source": [
        "# Data\r\n",
        "X = preprocessed_df[\"title\"].values \r\n",
        "y = preprocessed_df[\"category\"].values"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJxEUiLbjh4V",
        "outputId": "ebb93a76-f032-41cb-cd03-62f181f2f115"
      },
      "source": [
        "# Create data splits\r\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\r\n",
        "    X = X, y = y, train_size = TRAIN_SIZE)\r\n",
        "\r\n",
        "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\r\n",
        "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\r\n",
        "print (f\"X_train: {X_test.shape}, y_test: {y_test.shape}\")\r\n",
        "print (f\"Sample point: {X_train[0]} -> {y_train[0]}\")\r\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (84000,), y_train: (84000,)\n",
            "X_val: (18000,), y_val: (18000,)\n",
            "X_train: (18000,), y_test: (18000,)\n",
            "Sample point: china battles north korea nuclear talks -> World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtvU0BHUkSEr"
      },
      "source": [
        "# Label Encoding \r\n",
        "# to encode the text labels into unique indices\r\n",
        "\r\n",
        "import itertools"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2cewudKkgVY"
      },
      "source": [
        "class LabelEncoder(object):\r\n",
        "  \"\"\"Label encoder for tag labels.\"\"\"\r\n",
        "  def __init__(self, class_to_index = {}):\r\n",
        "    self.class_to_index = class_to_index\r\n",
        "    self.index_to_class = {v: k for k, v in self.class_to_index.items()}\r\n",
        "    self.classes = list(self.class_to_index.keys())\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.class_to_index)\r\n",
        "\r\n",
        "  def __str__(self):\r\n",
        "    return f\"<LabelEncoder(num_classes = {len(self)})>\"\r\n",
        "  \r\n",
        "  def fit(self, y):\r\n",
        "    classes = np.unique(y_train)\r\n",
        "    for i, class_ in enumerate(classes):\r\n",
        "      self.class_to_index[class_] = i\r\n",
        "    self.index_to_class = {v: k for k, v in self.class_to_index.items()}\r\n",
        "    self.classes = list(self.class_to_index.keys())\r\n",
        "    return self\r\n",
        "\r\n",
        "  def encode(self, y):\r\n",
        "    encoded = np.zeros((len(y)), dtype = int)\r\n",
        "    for i, item in enumerate(y):\r\n",
        "      encoded[i] = self.class_to_index[item]\r\n",
        "    return encoded\r\n",
        "\r\n",
        "  def decode(self, y):\r\n",
        "    classes = []\r\n",
        "    for i, item in enumerate(y):\r\n",
        "      classes.append(self.index_to_class[item])\r\n",
        "    return classes\r\n",
        "\r\n",
        "  def save(self, fp):\r\n",
        "    with open(fp, \"w\") as fp:\r\n",
        "      contents = {\"class_to_index\": self.class_to_index}\r\n",
        "      json.dump(contents, fp, indent = 4, sort_keys = False)\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def load(cls, fp):\r\n",
        "    with open(fp, \"r\") as fp:\r\n",
        "      kwargs = json.load(fp = fp)\r\n",
        "    return cls(**kwargs)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwntyBauo_QA",
        "outputId": "28e752ce-ac64-4cf3-fdd4-e242de7e0641"
      },
      "source": [
        "# Encode \r\n",
        "\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "label_encoder.fit(y_train)\r\n",
        "NUM_CLASSES = len(label_encoder)\r\n",
        "label_encoder.class_to_index"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHY1TClrpXP6",
        "outputId": "dc1ab4f3-49c2-4868-bc48-adf195731c36"
      },
      "source": [
        "# Converting labels to tokens \r\n",
        "\r\n",
        "print (f\"y_train[0]: {y_train[0]}\")\r\n",
        "\r\n",
        "y_train = label_encoder.encode(y_train)\r\n",
        "y_val = label_encoder.encode(y_val)\r\n",
        "y_test = label_encoder.encode(y_test)\r\n",
        "\r\n",
        "print (f\"y_train[0]: {y_train[0]}\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train[0]: World\n",
            "y_train[0]: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFHTuNzTp3sq",
        "outputId": "a79d96c7-2166-4beb-e8cd-d3f5d0290af3"
      },
      "source": [
        "# Class weights \r\n",
        "\r\n",
        "counts = np.bincount(y_train)\r\n",
        "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\r\n",
        "print (f\"counts: {counts}\\nweights: {class_weights}\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "counts: [21000 21000 21000 21000]\n",
            "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYDrlCrFrD_3"
      },
      "source": [
        "# Tokenizer \r\n",
        "\r\n",
        "import json\r\n",
        "from collections import Counter\r\n",
        "from more_itertools import take"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFX9mkPwriDZ"
      },
      "source": [
        "class Tokeniser(object):\r\n",
        "  def __init__(self, char_level, num_tokens = None,\r\n",
        "               pad_token = \"<PAD>\", oov_token = \"<UNK>\", \r\n",
        "               token_to_index = None):\r\n",
        "    self.char_level = char_level\r\n",
        "    self.separator = \" \" if self.char_level else \" \"\r\n",
        "    if num_tokens: num_tokens -= 2 # pad + unk tokens\r\n",
        "    self.num_tokens = num_tokens\r\n",
        "    self.oov_token = oov_token\r\n",
        "    if not token_to_index:\r\n",
        "      token_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\r\n",
        "    self.token_to_index = token_to_index\r\n",
        "    self.index_to_token = {v: k for k, v in self.token_to_index.items()}\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.token_to_index)\r\n",
        "\r\n",
        "  def __str__(self):\r\n",
        "    return f\"<Tokeniser(num_tokens = {len(self)})>\"\r\n",
        "\r\n",
        "  def fit_on_texts(self, texts):\r\n",
        "    if self.char_level:\r\n",
        "      all_tokens = [token for text in texts for token in text]\r\n",
        "    if not self.char_level:\r\n",
        "      all_tokens = [token for text in texts for token in text.split(\" \")]\r\n",
        "    counts = Counter(all_tokens).most_common(self.num_tokens)\r\n",
        "    self.min_token_freq = counts[-1][1]\r\n",
        "    for token, count in counts:\r\n",
        "      index = len(self)\r\n",
        "      self.token_to_index[token] = index\r\n",
        "      self.index_to_token[index] = token\r\n",
        "    return self\r\n",
        "\r\n",
        "  def texts_to_sequence(self, texts):\r\n",
        "    sequences = []\r\n",
        "    for text in texts:\r\n",
        "      if not self.char_level:\r\n",
        "        text = text.split(' ')\r\n",
        "      sequence = []\r\n",
        "      for token in text: \r\n",
        "        sequence.append(self.token_to_index.get(\r\n",
        "            token, self.token_to_index[self.oov_token]))\r\n",
        "      sequences.append(np.asarray(sequence))\r\n",
        "    return sequences\r\n",
        "  \r\n",
        "  def sequences_to_texts(self, sequences):\r\n",
        "    texts = []\r\n",
        "    for sequence in sequences:\r\n",
        "      text = []\r\n",
        "      for index in sequence :\r\n",
        "        text.append(self.index_to_token.get(index, self.oov_token))\r\n",
        "      texts.append(self.separator.join([token for token in text]))\r\n",
        "    return texts\r\n",
        "  \r\n",
        "  def save(self, fp):\r\n",
        "    with open(fp, \"w\") as fp:\r\n",
        "      contents = {\r\n",
        "          \"char_level\": self.char_level, \r\n",
        "          \"oov_token\": self.oov_token, \r\n",
        "          \"token_to_index\": self.token_to_index\r\n",
        "      }\r\n",
        "      json.dump(contents, fp, indent = 4, sort_keys = False)\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def load(cls, fp):\r\n",
        "    with open(fp, \"r\") as fp:\r\n",
        "      kwargs = json.load(fp = fp)\r\n",
        "    return cls(**kwargs)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTZBh3bAwQUL"
      },
      "source": [
        "## - we will restrict the number of tokens in our tokenizer to the top 500 most frequent tokens (stop words already removed)\r\n",
        "# -- because the full vocabulary (approx 30k) is too large to run on google colab\r\n",
        "\r\n",
        "# ** it is important that we are only using the training data split because during inference, the model will not always know every token\r\n",
        "# -- so it is important to replicate that scenario with the validation and test split. "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSQgaU3sxiWJ",
        "outputId": "3eb05a06-353a-47ef-e38f-289c9945a196"
      },
      "source": [
        "# Tokenise\r\n",
        "\r\n",
        "tokeniser = Tokeniser(char_level = False, num_tokens = 500)\r\n",
        "tokeniser.fit_on_texts(texts = X_train)\r\n",
        "VOCAB_SIZE = len(tokeniser)\r\n",
        "\r\n",
        "print (tokeniser)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Tokeniser(num_tokens = 500)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCShuyiwyXQc",
        "outputId": "45755081-7a5f-4ef3-b651-03e715c7768f"
      },
      "source": [
        "# Sample of tokens \r\n",
        "\r\n",
        "print (take(5, tokeniser.token_to_index.items()))\r\n",
        "print (f\"least freq tokens freq: {tokeniser.min_token_freq}\") # use this to adjust num tokens"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
            "least freq tokens freq: 166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y1tmtwozS5O",
        "outputId": "d8c3d869-7c17-48de-ad3a-d6d21dd796ca"
      },
      "source": [
        "# Convert texts to sequences of indices \r\n",
        "\r\n",
        "X_train = tokeniser.texts_to_sequence(X_train)\r\n",
        "X_val = tokeniser.texts_to_sequence(X_val)\r\n",
        "X_test = tokeniser.texts_to_sequence(X_test)\r\n",
        "\r\n",
        "preprocessed_text = tokeniser.sequences_to_texts([X_train[0]])[0]\r\n",
        "print (\"Text to indices: \\n\"\r\n",
        "    f\" (preprocessed) -> {preprocessed_text}\\n\"\r\n",
        "    f\" (tokenised) -> {X_train[0]}\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to indices: \n",
            " (preprocessed) -> china <UNK> north korea nuclear talks\n",
            " (tokenised) -> [ 16   1 285 142 114  24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXswfqRo0GmE"
      },
      "source": [
        "# One-hot Encoding \r\n",
        "\r\n",
        "# - Creates a binary column fro each unique value of each feature. \r\n",
        "# -- All the values for the token will be 0 except the index of that specific token"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG7EDYm513fS"
      },
      "source": [
        "def to_categorical(seq, num_classes):\r\n",
        "  \"\"\"One-hot encode a sequence of tokens.\"\"\"\r\n",
        "  one_hot = np.zeros((len(seq), num_classes))\r\n",
        "  for i, item in enumerate(seq):\r\n",
        "    one_hot[i, item] = 1.\r\n",
        "  return one_hot"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuH7iXsj2Ww1",
        "outputId": "264e0d85-b07b-4386-c8ea-f12d6879ca8a"
      },
      "source": [
        "# One-hot encoding \r\n",
        "print (X_train[0])\r\n",
        "print (len(X_train[0]))\r\n",
        "cat = to_categorical(seq = X_train[0], num_classes = len(tokeniser))\r\n",
        "\r\n",
        "print (cat)\r\n",
        "print (cat.shape)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 16   1 285 142 114  24]\n",
            "6\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(6, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bkcutvt2rQP"
      },
      "source": [
        "# Convert tokens to one-hot\r\n",
        "\r\n",
        "vocab_size = len(tokeniser)\r\n",
        "X_train = [to_categorical(seq, num_classes = vocab_size) for seq in X_train]\r\n",
        "X_val = [to_categorical(seq, num_classes = vocab_size) for seq in X_val]\r\n",
        "X_test = [to_categorical(seq, num_classes = vocab_size) for seq in X_test]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQQZ4P93Oy0"
      },
      "source": [
        "# Padding \r\n",
        "\r\n",
        "# - all the inputs have varying lengths, but each batch needs to e uniformly shaped\r\n",
        "# - we can use padding to make all the inputs in the batch the same length\r\n",
        "# - the padding index will be 0\r\n",
        "\r\n",
        "## ** one-hot encoding creates a batch of shape (N, max_seq_len, vocab_size) so we'll need to pad 3D sequences "
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFz7Go4m30W3"
      },
      "source": [
        "def pad_sequences(sequences, max_seq_len = 0):\r\n",
        "  \"\"\"Pad sequences to max length in sequence.\"\"\"\r\n",
        "  max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\r\n",
        "  num_classes = sequences[0].shape[-1]\r\n",
        "  padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))\r\n",
        "  for i, sequence in enumerate(sequences):\r\n",
        "    padded_sequences[i][:len(sequence)] = sequence\r\n",
        "  return padded_sequences"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHFOCr1H4rE_",
        "outputId": "ab7a63a7-64d2-4162-a15d-3db2ace7992a"
      },
      "source": [
        "# 3D sequences \r\n",
        "\r\n",
        "print (X_train[0].shape, X_train[1].shape, X_train[2].shape)\r\n",
        "padded = pad_sequences(X_train[0:3])\r\n",
        "print (padded.shape)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 500) (5, 500) (6, 500)\n",
            "(3, 6, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDFPyGyJ5CAT"
      },
      "source": [
        "# Dataset\r\n",
        "# - here we need to create datasets and dataloaders to be able to efficiently create batches with the data splits \r\n",
        "\r\n",
        "FILTER_SIZE = 1 # unigram"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTMezleJ6U7q"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, X, y, max_filter_size):\r\n",
        "    self.X = X\r\n",
        "    self.y = y\r\n",
        "    self.max_filter_size = max_filter_size\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.y)\r\n",
        "\r\n",
        "  def __str__(self):\r\n",
        "    return f\"<Dataset(N = {len(self)})>\"\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    X = self.X[index]\r\n",
        "    y = self.y[index]\r\n",
        "    return [X, y]\r\n",
        "\r\n",
        "  def collate_fn(self, batch):\r\n",
        "    \"\"\"Processing on batch.\"\"\"\r\n",
        "    # Get inputs\r\n",
        "    X = np.array(batch, dtype = object)[:, 0]\r\n",
        "    y = np.stack(np.array(batch, dtype = object)[:, 1], axis = 0)\r\n",
        "\r\n",
        "    # Pad sequences \r\n",
        "    X = pad_sequences(X, max_seq_len = self.max_filter_size)\r\n",
        "\r\n",
        "    # Cast\r\n",
        "    X = torch.FloatTensor(X.astype(np.int32))\r\n",
        "    y = torch.LongTensor(y.astype(np.int32))\r\n",
        "\r\n",
        "    return X, y\r\n",
        "\r\n",
        "  def create_dataloader(self, batch_size, shuffle = False, drop_last = False):\r\n",
        "    return torch.utils.data.DataLoader(\r\n",
        "        dataset = self, batch_size = batch_size, collate_fn = self.collate_fn, \r\n",
        "        shuffle = shuffle, drop_last = drop_last, pin_memory = True)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXoY27PZ8Q5u",
        "outputId": "82888948-1fa7-4189-ef1e-1405f77d15ac"
      },
      "source": [
        "# Create datasets for embedding \r\n",
        "\r\n",
        "train_dataset = Dataset(X = X_train, y = y_train, max_filter_size = FILTER_SIZE)\r\n",
        "val_dataset = Dataset(X = X_val, y = y_val, max_filter_size = FILTER_SIZE)\r\n",
        "test_dataset = Dataset(X = X_test, y = y_test, max_filter_size = FILTER_SIZE)\r\n",
        "print (\"Datasets: \\n\"\r\n",
        "    f\" Train dataset: {train_dataset.__str__()}\\n\"\r\n",
        "    f\" Val dataset: {val_dataset.__str__()}\\n\"\r\n",
        "    f\" Test dataset: {test_dataset.__str__()}\\n\"\r\n",
        "    \"Sample Point: \\n\"\r\n",
        "    f\" X: {test_dataset[0][0]}\\n\"\r\n",
        "    f\" y: {test_dataset[0][1]}\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datasets: \n",
            " Train dataset: <Dataset(N = 84000)>\n",
            " Val dataset: <Dataset(N = 18000)>\n",
            " Test dataset: <Dataset(N = 18000)>\n",
            "Sample Point: \n",
            " X: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            " y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En480uM4-uS7",
        "outputId": "7fc61005-232f-438b-cebf-d981f2412049"
      },
      "source": [
        "# Create dataloaders \r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "train_dataloader = train_dataset.create_dataloader(batch_size = batch_size)\r\n",
        "val_dataloader = val_dataset.create_dataloader(batch_size = batch_size)\r\n",
        "test_dataloader = test_dataset.create_dataloader(batch_size = batch_size)\r\n",
        "batch_X, batch_y = next(iter(test_dataloader))\r\n",
        "print (\"Sample batch:\\n\"\r\n",
        "    f\" X: {list(batch_X.size())}\\n\"\r\n",
        "    f\" y: {list(batch_y.size())}\\n\"\r\n",
        "    \"Sample point:\\n\"\r\n",
        "    f\" X: {batch_X[0]}\\n\"\r\n",
        "    f\" y: {batch_y[0]}\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample batch:\n",
            " X: [64, 14, 500]\n",
            " y: [64]\n",
            "Sample point:\n",
            " X: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cpu')\n",
            " y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3T_oqa4DcoL"
      },
      "source": [
        "# CNN - Convolutional Neural Networks \r\n",
        "\r\n",
        "# - Here we will learn about CNNs by applying them on 1D text data\r\n",
        "# In the example below, we have a batch of N samples where wach sample has 8 characters and each char represented by an array of 10 values (vocab size = 10)\r\n",
        "# - this gives our inputs the size (N, 8, 10)\r\n",
        "\r\n",
        "# -- with PyTorch, when dealing with convs, the inputs X need to have the channels as the second dimension, so our inputs will be (N,10,8)\r\n",
        "\r\n",
        "import math\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfdX2dWkG4KD",
        "outputId": "a37f8ee2-ad29-446f-b3a7-53c8b0a90172"
      },
      "source": [
        "# Assume all our inputs are padded to have the same words\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "max_seq_len = 8 # words per input\r\n",
        "vocab_size = 10 # one hot size\r\n",
        "x = torch.randn(batch_size, max_seq_len, vocab_size)\r\n",
        "print (f\" X:{x.shape}\")\r\n",
        "x = x.transpose(1, 2)\r\n",
        "print (f\" X:{x.shape}\")"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " X:torch.Size([64, 8, 10])\n",
            " X:torch.Size([64, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flM_mMvwH80y"
      },
      "source": [
        "# At the core of CNNs are filters, also known as weights, kernels etc), which convolve (slide) across our input to extract relevant features. \r\n",
        "# the filters are initialised randomly but learn to act as feature extractors via parameter sharing.\r\n",
        "\r\n",
        "# We will use a conv1d layer to process our inouts "
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGCgRgglLTMD",
        "outputId": "2e8db75d-08f2-4487-b647-2e35a433c2d6"
      },
      "source": [
        "# Convolutional filters (Valid padding)\r\n",
        "\r\n",
        "vocab_size = 10 # one-hot size\r\n",
        "num_filters = 50 # num of filters \r\n",
        "filter_size = 3 # filters are 3 X 3 \r\n",
        "stride = 1\r\n",
        "padding = 0 # valid padding (no padding)\r\n",
        "conv1 = nn.Conv1d(in_channels = vocab_size, out_channels = num_filters, \r\n",
        "                  kernel_size = filter_size, stride = stride, \r\n",
        "                  padding = padding, padding_mode = \"zeros\")\r\n",
        "print (\"conv: {}\".format(conv1.weight.shape))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv: torch.Size([50, 10, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBK_HURrMKJl",
        "outputId": "7dfc7a75-415f-4078-97e2-ee3c16a6a4fa"
      },
      "source": [
        "# Forward pass\r\n",
        "z = conv1(x)\r\n",
        "print (f\"z: {z.shape}\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z: torch.Size([64, 50, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ3NIvVtMTjV"
      },
      "source": [
        "# Now, we'll add padding so that the convolutional outputs are the same shape as our inputs. \r\n",
        "# - we want our output to have the same width as our input. "
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVMB5lgXNPVT",
        "outputId": "c53fcf9a-8649-41b2-f9de-15c3f49e22e2"
      },
      "source": [
        "# Convolutional filters (Same padding)\r\n",
        "\r\n",
        "vocab_size = 10 # one-hot size\r\n",
        "num_filters = 50 # num filters \r\n",
        "filter_size = 3 # filters are 3 X 3\r\n",
        "stride = 1\r\n",
        "conv = nn.Conv1d(in_channels = vocab_size, out_channels = num_filters, \r\n",
        "                 kernel_size = filter_size, stride = stride)\r\n",
        "print (\"conv: {}\".format(conv.weight.shape))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv: torch.Size([50, 10, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqdlRQTuN4LV",
        "outputId": "82987aaf-ff28-4042-f37a-918142a112c4"
      },
      "source": [
        "# Same padding \r\n",
        "\r\n",
        "padding_left = int((conv.stride[0] * (max_seq_len-1) - max_seq_len + filter_size) / 2)\r\n",
        "padding_right = int(math.ceil((conv.stride[0] * (max_seq_len - 1) - max_seq_len + filter_size) / 2))\r\n",
        "print (f\"padding: {(padding_left, padding_right)}\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padding: (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEGUkVCgdMmT",
        "outputId": "726f17a6-e115-40d8-e39c-1a5c3f7e6424"
      },
      "source": [
        "# Forward pass \r\n",
        "\r\n",
        "z = conv(F.pad(x, (padding_left, padding_right)))\r\n",
        "print (f\"z: {z.shape}\")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z: torch.Size([64, 50, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOMcR8txdk-g",
        "outputId": "f39e9b4b-355a-4d13-abeb-6f5ac7124966"
      },
      "source": [
        "# Pooling \r\n",
        "\r\n",
        "# - the result of the convolving filters on an input is a feature map. Due to the nature of convolution and overlaps,our feature map will have lots of redundant information.\r\n",
        "# - Pooling is a way to summarise a high-dimensional feature map into a lower dimensional one for simplified downstream computation. \r\n",
        "# -- the pooling operation can be the max value, average etc\r\n",
        "\r\n",
        "# Max pooling \r\n",
        "\r\n",
        "pool_output = F.max_pool1d(z, z.size(2))\r\n",
        "print (\"Size: {}\".format(pool_output.shape))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([64, 50, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZSujn3Efqiv"
      },
      "source": [
        "# Batch normalisation \r\n",
        "\r\n",
        "# - this is an operation that will standardise the activations from the previous layer \r\n",
        "# recall that we've previously standardised our inputs so that the model can optimise quickly with larger learning learning rates\r\n",
        "# - here we will use the same concept  but we will continue to maintain standardised values throughout the forward pass to further aid optimisation"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8TyhVdkR7N",
        "outputId": "60dc6469-9298-4131-fa39-e08befe4bdc5"
      },
      "source": [
        "batch_norm = nn.BatchNorm1d(num_features = num_filters)\r\n",
        "z = batch_norm(conv(x)) # applied to activations (after conv layer & before pooling)\r\n",
        "print (f\"z: {z.shape}\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z: torch.Size([64, 50, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8bwjkmvkp8N",
        "outputId": "5957626a-e619-4fc6-ff31-1a37bf31bef9"
      },
      "source": [
        "# Mean and std before batchnorm\r\n",
        "print (f\"mean: {torch.mean(conv1(x)):.2f}, std: {torch.std(conv(x)):.2f}\")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: 0.01, std: 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVnRsWN4lCoa",
        "outputId": "1a36fb60-06bb-4bc7-f444-2af4e3046a38"
      },
      "source": [
        "# Mean and std after batchnorm\r\n",
        "print (f\"mean: {torch.mean(z):.2f}, std: {torch.std(z):.2f}\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: 0.00, std: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dRV-VYzlSMQ"
      },
      "source": [
        "# Modelling \r\n",
        "\r\n",
        "# First, tokenise the inputs (batch_size, max_seq_len)\r\n",
        "# Then, one-hot encode the tokenised inputs(batch_size, max_seq_len, vocab_size)\r\n",
        "# After, apply, conv filters (filter_size, vocab_size, num_filters) then, batch norm\r\n",
        "# Then, apply 1D global max pooling which will extract the most rlevant information from th feature maps for making the decision\r\n",
        "# After, feed the pool outputs to a fully connected layer with dropout\r\n",
        "# Last, we use one more fully connected layer with Softmax to derive class probabilities"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23oWYx7pwRv"
      },
      "source": [
        "NUM_FILTERS = 50\r\n",
        "HIDDEN_DIM = 100\r\n",
        "DROPOUT_P = 0.1"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im1D9qFrs3Qm"
      },
      "source": [
        "class CNN(nn.Module):\r\n",
        "  def __init__(self, vocab_size, num_filters, filter_size, hidden_dim, dropout_p, num_classes):\r\n",
        "    super(CNN, self).__init__()\r\n",
        "\r\n",
        "    # Convolutional filters \r\n",
        "    self.filter_size = filter_size\r\n",
        "    self.conv = nn.Conv1d(\r\n",
        "        in_channels = vocab_size, out_channels = num_filters, \r\n",
        "        kernel_size = filter_size, stride = 1, padding = 0, padding_mode = \"zeros\")\r\n",
        "    self.batch_norm = nn.BatchNorm1d(num_features = num_filters)\r\n",
        "\r\n",
        "    # Fully Connected layers\r\n",
        "    self.fc1 = nn.Linear(num_filters, hidden_dim)\r\n",
        "    self.dropout_p = nn.Dropout(dropout_p)\r\n",
        "    self.fc2 = nn.Linear(hidden_dim, num_classes)\r\n",
        "\r\n",
        "  def forward(self, inputs, channel_first = False, apply_softmax = False):\r\n",
        "    # Rearrange input so num_channels is in dim 1 (N, C, L)\r\n",
        "    x_in, = inputs\r\n",
        "    if not channel_first:\r\n",
        "      x_in = x_in.transpose(1, 2)\r\n",
        "\r\n",
        "    # Padding for SAME padding \r\n",
        "    max_seq_len = x_in.shape[2]\r\n",
        "    padding_left = int((self.conv.stride[0] * (max_seq_len -1) - max_seq_len + self.filter_size) / 2)\r\n",
        "    padding_right = int(math.ceil((self.conv.stride[0] * (max_seq_len) - max_seq_len + self.filter_size) / 2))\r\n",
        "\r\n",
        "    # Conv outputs \r\n",
        "    z = self.conv(F.pad(x_in, (padding_left, padding_right))) \r\n",
        "    z = F.max_pool1d(z, z.size(2)).squeeze(2)\r\n",
        "\r\n",
        "    # Fully connected layer \r\n",
        "    z = self.fc1(z)\r\n",
        "    z = self.dropout_p(z)\r\n",
        "    y_pred = self.fc2(z)\r\n",
        "\r\n",
        "    if apply_softmax:\r\n",
        "      y_pred = F.softmax(y_pred, dim = 1)\r\n",
        "    return y_pred"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpyB3YRLwYHa",
        "outputId": "544e162a-c4e6-427a-fc3f-b3f026108e4f"
      },
      "source": [
        "# Initialising the model \r\n",
        "\r\n",
        "model = CNN(vocab_size = VOCAB_SIZE, num_filters = NUM_FILTERS, filter_size = FILTER_SIZE,\r\n",
        "            hidden_dim = HIDDEN_DIM,dropout_p = DROPOUT_P, num_classes = NUM_CLASSES )\r\n",
        "model = model.to(device) # set device\r\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of CNN(\n",
            "  (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))\n",
            "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=50, out_features=100, bias=True)\n",
            "  (dropout_p): Dropout(p=0.1, inplace=False)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8chLlkLP45Wr"
      },
      "source": [
        "# Training \r\n",
        "\r\n",
        "from torch.optim import Adam"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z40Gq1Ff5FNR"
      },
      "source": [
        "LEARNING_RATE = 1e-3\r\n",
        "PATIENCE = 5\r\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w3jMf615MlO"
      },
      "source": [
        "class Trainer(object):\r\n",
        "  def __init__(self, model, device, loss = None, optimiser = None, scheduler = None):\r\n",
        "    \r\n",
        "    # Set params \r\n",
        "    self.model = model \r\n",
        "    self.device = device \r\n",
        "    self.loss = loss\r\n",
        "    self.optimiser = optimiser\r\n",
        "    self.scheduler = scheduler\r\n",
        "\r\n",
        "  def train_step(self, dataloader):\r\n",
        "    \"\"\"Train step.\"\"\"\r\n",
        "    # Set model to train mode \r\n",
        "    self.model.train()\r\n",
        "    loss = 0.0\r\n",
        "\r\n",
        "    # Iterate over train batches\r\n",
        "    for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "      # Step\r\n",
        "      batch = [item.to(self.device) for item in batch] # Set device \r\n",
        "      inputs, targets = batch[:-1], batch[-1]\r\n",
        "      self.optimiser.zero_grad() # Reset gradients \r\n",
        "      z = self.model(inputs) # Forward pass\r\n",
        "      J = self.loss(z, targets) # Define loss\r\n",
        "      J.backward() # Backward pass \r\n",
        "      self.optimiser.step() # Update weights\r\n",
        "\r\n",
        "      # Cumulative metrics \r\n",
        "      loss += (J.detach().item() - loss) / (i + 1)\r\n",
        "\r\n",
        "    return loss\r\n",
        "  \r\n",
        "  def eval_step(self, dataloader):\r\n",
        "    \"\"\"Validation of test step.\"\"\"\r\n",
        "    # Set model to eval mode\r\n",
        "    self.model.eval()\r\n",
        "    loss = 0.0\r\n",
        "    y_trues, y_probs = [], []\r\n",
        "\r\n",
        "    # Iterate over the val batches \r\n",
        "    with torch.no_grad():\r\n",
        "      for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "        # Step\r\n",
        "        batch = [item.to(self.device) for item in batch] # Set device\r\n",
        "        inputs, y_true = batch[:-1], batch[-1]\r\n",
        "        z = self.model(inputs) # Forward pass\r\n",
        "        J = self.loss(z, y_true).item()\r\n",
        "\r\n",
        "        # Cumulative Metrics \r\n",
        "        loss += (J - loss) / (i + 1)\r\n",
        "\r\n",
        "        # Store outputs \r\n",
        "        y_prob = torch.sigmoid(z).cpu().numpy()\r\n",
        "        y_probs.extend(y_prob)\r\n",
        "        y_trues.extend(y_true.cpu().numpy())\r\n",
        "    \r\n",
        "    return loss, np.vstack(y_trues), np.vstack(y_probs)\r\n",
        "\r\n",
        "  def predict_step(self, dataloader):\r\n",
        "    \"\"\"Prediction step.\"\"\"\r\n",
        "    # Set model to eval mode\r\n",
        "    self.model.eval()\r\n",
        "    y_probs = []\r\n",
        "\r\n",
        "    # Iterate over the val batches\r\n",
        "    with torch.no_grad():\r\n",
        "      for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "        # Forward pass w/ inputs \r\n",
        "        inputs, targets = batch[:-1], batch[-1]\r\n",
        "        y_prob = self.model(inputs, apply_softmax = True)\r\n",
        "\r\n",
        "        # Store outputs\r\n",
        "        y_probs.extend(y_prob)\r\n",
        "\r\n",
        "    return np.vstack(y_probs)\r\n",
        "\r\n",
        "  def train(self, num_epochs, patience, train_dataloader, val_dataloader):\r\n",
        "    best_val_loss = np.inf\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "\r\n",
        "      # Steps\r\n",
        "      train_loss = self.train_step(dataloader = train_dataloader)\r\n",
        "      val_loss, _, _ = self.eval_step(dataloader = val_dataloader)\r\n",
        "\r\n",
        "      # Early stopping\r\n",
        "      if val_loss < best_val_loss:\r\n",
        "        best_val_loss = val_loss\r\n",
        "        best_model = self.model\r\n",
        "        _patience = patience # Reset patience \r\n",
        "      else:\r\n",
        "        _patience -= 1\r\n",
        "      if not _patience: # 0\r\n",
        "        print (\"Stopping early!\")\r\n",
        "        break\r\n",
        "\r\n",
        "      # Logging\r\n",
        "      print (\r\n",
        "          f\"Epoch: {epoch +1} | \"\r\n",
        "          f\"train_loss: {train_loss:.5f}, \"\r\n",
        "          f\"val_loss: {val_loss:.5f}, \"\r\n",
        "          f\"lr: {self.optimiser.param_groups[0]['lr']:.2E}, \"\r\n",
        "          f\"_patience: {_patience}\"\r\n",
        "      )\r\n",
        "    return best_model\r\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6DrujMEBIrt"
      },
      "source": [
        "# Define loss\r\n",
        "\r\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\r\n",
        "loss = nn.CrossEntropyLoss(weight = class_weights_tensor)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaOx3-5sBrkM"
      },
      "source": [
        "# Define optimiser and scheduler\r\n",
        "\r\n",
        "optimiser = Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n",
        "    optimiser, mode = 'min', factor = 0.1, patience = 3)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdHfoM7eCEdY"
      },
      "source": [
        "# Trainer module\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model = model, device = device, loss = loss, \r\n",
        "    optimiser = optimiser, scheduler = scheduler)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4mJ6Mm5EyPd",
        "outputId": "fc0354db-0f01-4177-8dd1-37048971a2c2"
      },
      "source": [
        "# Train\r\n",
        "best_model = trainer.train(\r\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.87454, val_loss: 0.79113, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 2 | train_loss: 0.78471, val_loss: 0.78694, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 3 | train_loss: 0.77998, val_loss: 0.78519, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 4 | train_loss: 0.77678, val_loss: 0.78441, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 5 | train_loss: 0.77483, val_loss: 0.78339, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 6 | train_loss: 0.77326, val_loss: 0.78268, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 7 | train_loss: 0.77227, val_loss: 0.78248, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 8 | train_loss: 0.77135, val_loss: 0.78251, lr: 1.00E-03, _patience: 4\n",
            "Epoch: 9 | train_loss: 0.77042, val_loss: 0.78203, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 10 | train_loss: 0.76958, val_loss: 0.78201, lr: 1.00E-03, _patience: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O5TxEf2GIhX"
      },
      "source": [
        "# Evaluation \r\n",
        "\r\n",
        "import json\r\n",
        "from pathlib import Path\r\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj7zc1tEKVeF"
      },
      "source": [
        "def get_performance(y_true, y_pred, classes):\r\n",
        "  \"\"\"Per-class performance metrics.\"\"\"\r\n",
        "  # Get metrics \r\n",
        "  performance = {\"overall\": {}, \"class\": {}}\r\n",
        "  metrics = precision_recall_fscore_support(y_true, y_pred)\r\n",
        "\r\n",
        "  # Overall performance \r\n",
        "  performance[\"overall\"][\"precision\"] = np.mean(metrics[0])\r\n",
        "  performance[\"overall\"][\"recall\"] = np.mean(metrics[1])\r\n",
        "  performance[\"overall\"][\"f1\"] = np.mean(metrics[2])\r\n",
        "  performance[\"overall\"][\"num_samples\"] = np.float64(np.sum(metrics[3]))\r\n",
        "\r\n",
        "  # Per class performance\r\n",
        "  for i in range(len(classes)):\r\n",
        "    performance[\"class\"][classes[i]] = {\r\n",
        "        \"precision\": metrics[0][i], \r\n",
        "        \"recall\": metrics[1][i], \r\n",
        "        \"f1\": metrics[2][i], \r\n",
        "        \"num_samples\": np.float64(metrics[3][i])\r\n",
        "    }\r\n",
        "  return performance"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3ZmkT-ANPGj"
      },
      "source": [
        "# get predictions \r\n",
        "\r\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader = test_dataloader)\r\n",
        "y_pred = np.argmax(y_prob, axis = 1)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvhiJNiQNcTF",
        "outputId": "cbe1dae8-75c1-426f-a790-ae13762ae057"
      },
      "source": [
        "# Determine performance \r\n",
        "performance = get_performance(\r\n",
        "    y_true = y_test, y_pred = y_pred, classes = label_encoder.classes)\r\n",
        "print (json.dumps(performance[\"overall\"], indent = 2))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.7115579309819231,\n",
            "  \"recall\": 0.6917222222222221,\n",
            "  \"f1\": 0.6911547384674819,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q-jC-2aNyxP"
      },
      "source": [
        "# Save artifacts \r\n",
        "\r\n",
        "dir = Path(\"cnn\")\r\n",
        "dir.mkdir(parents = True, exist_ok = True)\r\n",
        "label_encoder.save(fp = Path(dir, \"label_encoder.json\"))\r\n",
        "tokeniser.save(fp = Path(dir, \"tokeniser.json\"))\r\n",
        "torch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\r\n",
        "with open(Path(dir, \"performance.json\"), \"w\") as fp:\r\n",
        "  json.dump(performance, indent = 2, sort_keys = False, fp = fp)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6waZLEpO18N"
      },
      "source": [
        "# Inference \r\n",
        "def get_probability_distribution(y_prob, classes):\r\n",
        "  \"\"\"Create a dict of class probabilities from an array.\"\"\"\r\n",
        "  results = {}\r\n",
        "  for i, class_ in enumerate(classes):\r\n",
        "    results[class_] = np.float64(y_prob[i])\r\n",
        "  sorted_results = {k: v for k, v in sorted(\r\n",
        "      results.items(), key = lambda item: item[1], reverse = True)}\r\n",
        "  return sorted_results\r\n"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vi-1OmuQy34",
        "outputId": "8c1e7cda-9e70-4008-893c-4ad08d7eb376"
      },
      "source": [
        "# Load artifacts\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "label_encoder = LabelEncoder.load(fp=Path(dir, 'label_encoder.json'))\r\n",
        "tokeniser = Tokeniser.load(fp=Path(dir, 'tokeniser.json'))\r\n",
        "model = CNN(\r\n",
        "    vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,\r\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\r\n",
        "model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\r\n",
        "model.to(device)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))\n",
              "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=50, out_features=100, bias=True)\n",
              "  (dropout_p): Dropout(p=0.1, inplace=False)\n",
              "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ML4nTkSnVq"
      },
      "source": [
        "# Initialise trainer \r\n",
        "\r\n",
        "trainer = Trainer(model = model, device = device)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOsiO8GOWKWa",
        "outputId": "12537b02-65eb-4c95-e0e2-0e058a6aa9bf"
      },
      "source": [
        "# Dataloader \r\n",
        "\r\n",
        "text = \"What a day for the new york stock market to go burst!\"\r\n",
        "sequences = tokeniser.texts_to_sequence([preprocess(text)])\r\n",
        "print (tokeniser.sequences_to_texts(sequences))\r\n",
        "X = [to_categorical(seq, num_classes = len(tokeniser)) for seq in sequences]\r\n",
        "y_filler = label_encoder.encode([label_encoder.classes[0]] * len(X))\r\n",
        "dataset = Dataset(X = X, y = y_filler, max_filter_size = FILTER_SIZE)\r\n",
        "dataloader = dataset.create_dataloader(batch_size = batch_size)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['day new <UNK> stock market go <UNK>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf7AubVYXGzu",
        "outputId": "8473eb64-33aa-44aa-9a3b-5b2f9cc1b4bb"
      },
      "source": [
        "# Inference\r\n",
        "\r\n",
        "y_prob = trainer.predict_step(dataloader)\r\n",
        "y_pred = np.argmax(y_prob, axis = 1)\r\n",
        "label_encoder.decode(y_pred)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Business']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkpHPBDPXp9Z",
        "outputId": "cd6128e0-2e12-45cf-fab6-3ddc14bb664a"
      },
      "source": [
        "# Class distributions\r\n",
        "\r\n",
        "prob_dist = get_probability_distribution(y_prob = y_prob[0], classes = label_encoder.classes)\r\n",
        "print (json.dumps(prob_dist, indent = 2))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Business\": 0.8190300464630127,\n",
            "  \"Sci/Tech\": 0.16043992340564728,\n",
            "  \"World\": 0.014267615973949432,\n",
            "  \"Sports\": 0.006262409966439009\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyaVQzkAX-vf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}