{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Back to Basics: Embeddings",
      "provenance": [],
      "authorship_tag": "ABX9TyO8aAx0Gg4GmlG0764Vv1go",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Otobi1/Back-to-Basics-A-Refresher-/blob/master/Back_to_Basics_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiBz5ORdbyn"
      },
      "source": [
        "# Embeddings \r\n",
        "# - these are capable of capturing the contextual, semantic and syntatic meaning in data.\r\n",
        "\r\n",
        "# While one-hot encoding allows us to preserve the structural info, it has two disadvantages \r\n",
        "# 1. linearly dependent on the number of unique token in the vocab which is problem if we have a large corpus\r\n",
        "# 2. the representation for each token does not preserve any relationship with respect to other tokens\r\n",
        "\r\n",
        "# Embeddings address the short comings of one-hot encoding\r\n",
        "# - its main idea is to have fixed length representations for the tokens in a text regardless of the tokens in the vocab\r\n",
        "# with one-hot encoding, each token is represented by an array of size vocab size but with embeddings, each token now has the shape embed dim\r\n",
        "###- the values in the rep are not fixed binary values but rather changing floating points allowing for fine-grained learned reps\r\n",
        "\r\n",
        "# the objective here is to rep tokens in text that capture the intrinsic semantic relationships\r\n",
        "## leveraging the low-dimensionality while capturing relationships and interpretable token reps."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhvs0QObfR5u",
        "outputId": "bc3f0387-439b-4c7c-e7b5-90ebce52416b"
      },
      "source": [
        "# Learning Embeddings \r\n",
        "# - We can learn embeddings by creating our model in PyTorch, but first, we're going to use a library that specialises in embeddings and topic modelling called Gensim\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt');\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import urllib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQot3XjrfxM_"
      },
      "source": [
        "SEED = 1234"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU7tcZAXf8Ff"
      },
      "source": [
        "# Set seed for reproducibility \r\n",
        "\r\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9KVbUSxgA91",
        "outputId": "c941d68c-24d2-45dd-ee0c-58ad431c483c"
      },
      "source": [
        "# Split text into sentences \r\n",
        "\r\n",
        "tokeniser = nltk.data.load(\"tokenizers/punkt/english.pickle\")\r\n",
        "book = urllib.request.urlopen(url = \"https://raw.githubusercontent.com/GokuMohandas/madewithml/main/datasets/harrypotter.txt\")\r\n",
        "sentences = tokeniser.tokenize(str(book.read()))\r\n",
        "print (f\"{len(sentences)} sentences\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12443 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zpt221jgv4w"
      },
      "source": [
        "def preprocess(text):\r\n",
        "  \"\"\"Conditional preprocessing on our text.\"\"\"\r\n",
        "  # Lower\r\n",
        "  text = text.lower()\r\n",
        "\r\n",
        "  # Spacing and filters \r\n",
        "  text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\r\n",
        "  text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric characters \r\n",
        "  text = re.sub(' +', ' ', text) # remove multiple spaces\r\n",
        "  text = text.strip()\r\n",
        "\r\n",
        "  # Separate into word tokens\r\n",
        "  text = text.split(\" \")\r\n",
        "\r\n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHlGfLTmh5vH",
        "outputId": "e608881e-aa8b-433d-ff84-0dae24bff465"
      },
      "source": [
        "# Preprocess sentences\r\n",
        "print (sentences[11])\r\n",
        "sentences = [preprocess(sentence) for sentence in sentences]\r\n",
        "print (sentences[11])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Snape nodded, but did not elaborate.\n",
            "['snape', 'nodded', 'but', 'did', 'not', 'elaborate']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjvvySmmiPEz"
      },
      "source": [
        "# How doe we learn the embeddings in the first place?\r\n",
        "# The intuition behind embeddins is that the definition of a token depends NOT on the token itself, but on its context.\r\n",
        "# - There are several ways of doing this. \r\n",
        "# -- given the word in context, predict the target word (CBOW - continous bag of words)\r\n",
        "# -- given the target word, predict the context word (skip-gram)\r\n",
        "# -- given a sequence of words, predict the next word(LM - language modelling)\r\n",
        "\r\n",
        "# all these approaches involve the creation of data to train the model on. \r\n",
        "# Every word in a sentence becomes the target word and the context words are determined by a window\r\n",
        "# we repeat this for every sentence in the corpus and this results in the training data for unsupervised taskk\r\n",
        "\r\n",
        "# the idea is that similar target words will appear with similar contexts and we can learn this relationship by repeatedly training our model (wiht context and target) pairs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "450GRspwiZcU"
      },
      "source": [
        "# Word2Vec\r\n",
        "\r\n",
        "# working with large vocabs to learn embeddings can become complicated quickly\r\n",
        "# Here, we can use the \"negative sampling\", which only updates the correct class and a few arbitrary incorrect classes \r\n",
        "# We can do this because of the large amoutn of training data where we will see the same word as the target class multiple times\r\n",
        "\r\n",
        "import gensim\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qup89Kc8lYFA"
      },
      "source": [
        "EMBEDDING_DIM = 100\r\n",
        "WINDOW = 5\r\n",
        "MIN_COUNT = 3 # ignores all the words with total frequency lower than this\r\n",
        "SKIP_GRAM = 1 # 0 = CBOW\r\n",
        "NEGATIVE_SAMPLING = 20"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcJuTmY3lqkf",
        "outputId": "7d6f671e-f9dc-472c-929a-b546cab50ac0"
      },
      "source": [
        "# super fast because of optimised C code under the hood\r\n",
        "\r\n",
        "w2v = Word2Vec(sentences = sentences, size = EMBEDDING_DIM, window = WINDOW,\r\n",
        "               min_count = MIN_COUNT, sg = SKIP_GRAM, negative = NEGATIVE_SAMPLING)\r\n",
        "print (w2v)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=4937, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKBOHpTxmEJc",
        "outputId": "1eb3a6fc-0c58-42fd-b2f2-9d74a09f57c8"
      },
      "source": [
        "# Vector for each word\r\n",
        "w2v.wv.get_vector(\"potter\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.14101067e-01, -1.86282560e-01, -1.96112245e-01, -4.37779844e-01,\n",
              "       -2.18696296e-01,  6.13153130e-02, -2.68156797e-01,  5.49259856e-02,\n",
              "        3.87510538e-01,  3.57815892e-01, -4.57724869e-01, -2.98481286e-01,\n",
              "       -2.44551778e-01, -1.48118595e-02, -1.39958128e-01, -3.70013803e-01,\n",
              "        1.70049042e-01,  1.82302162e-01, -5.09113669e-01,  1.95511967e-01,\n",
              "       -2.11835250e-01, -3.96204233e-01, -1.62837163e-01,  2.33413607e-01,\n",
              "       -2.88501829e-01,  5.16816020e-01, -2.68738568e-02,  2.61840940e-01,\n",
              "       -6.31084368e-02, -2.39665210e-01,  5.61609209e-01,  3.76967430e-01,\n",
              "       -5.10030724e-02, -2.95698971e-01, -3.10349941e-01,  2.53110495e-03,\n",
              "        2.99663901e-01, -2.41563305e-01, -2.20411971e-01, -1.10006832e-01,\n",
              "       -9.20895562e-02, -1.83540471e-02, -1.59294441e-01, -2.67816428e-02,\n",
              "        2.46242791e-01,  3.71243924e-01, -8.76362324e-02, -1.73952013e-01,\n",
              "        4.29101586e-02, -4.38437164e-01,  1.54941484e-01,  3.31484169e-01,\n",
              "        7.01173991e-02,  1.94590300e-01,  7.83513859e-02, -9.72923040e-02,\n",
              "        3.09282213e-01, -1.11021914e-01,  1.08024172e-01, -3.87210906e-01,\n",
              "        1.07682779e-01, -1.42027959e-01, -2.39661396e-01, -1.45087436e-01,\n",
              "        2.41001323e-01, -8.59495923e-02, -7.70505518e-02,  8.06622684e-01,\n",
              "        4.51026764e-03, -9.22237262e-02, -1.42298192e-01, -4.19827282e-01,\n",
              "       -1.61890358e-01,  1.81203470e-01,  1.46908894e-01,  3.48367393e-02,\n",
              "        1.13557942e-01,  1.08289495e-01, -2.10339010e-01, -2.88670752e-02,\n",
              "        1.73797041e-01, -1.90468505e-01,  7.74471387e-02, -4.38105762e-01,\n",
              "        2.79968679e-02, -1.11408189e-01, -1.83019832e-01,  9.73317102e-02,\n",
              "       -5.10798544e-02,  1.13982864e-01,  1.13423273e-01, -4.65391582e-04,\n",
              "       -4.56303284e-02, -2.54516572e-01,  1.51062295e-01, -7.05037639e-02,\n",
              "        1.72467887e-01,  2.31113404e-01, -2.89231181e-01,  1.88723430e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fXaF0SnmuzX",
        "outputId": "0f14eff3-aa34-495e-eb63-f74937d28b2b"
      },
      "source": [
        "# Get nearest neighbours (excluding itself)\r\n",
        "\r\n",
        "w2v.wv.most_similar(positive = \"scar\", topn = 5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pain', 0.9308451414108276),\n",
              " ('forehead', 0.9219275116920471),\n",
              " ('prickling', 0.9136929512023926),\n",
              " ('cold', 0.9049762487411499),\n",
              " ('mouth', 0.9016754031181335)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la5RNaihm9sk"
      },
      "source": [
        "# Saving and loading \r\n",
        "\r\n",
        "w2v.wv.save_word2vec_format(\"model.bin\", binary = True)\r\n",
        "w2v = KeyedVectors.load_word2vec_format(\"model.bin\", binary = True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcmQyBXHnVxx"
      },
      "source": [
        "# FastText\r\n",
        "\r\n",
        "# What happens if a word doesnt exist in the vocab?\r\n",
        "# We could assign an \"UNK\" (unkown) token which is used for all OOV (out of vocab) words or we could use FastText, which uses character-level n-grams to embed a word\r\n",
        "# This helps embed rare words, misspelled words and also words that don't exist in our corpus but are similar to words in our corpus"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeI_6Qy-oE8W"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4GHCas5oJD0",
        "outputId": "dcab4b64-2d9f-478e-da9a-0c1601da2ffa"
      },
      "source": [
        "# super fast because of the optimised C code under the hood\r\n",
        "\r\n",
        "ft = FastText(sentences = sentences, size = EMBEDDING_DIM, window = WINDOW, \r\n",
        "              min_count = MIN_COUNT, sg = SKIP_GRAM, negative = NEGATIVE_SAMPLING)\r\n",
        "print (ft)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FastText(vocab=4937, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt2OJ2IeojP_"
      },
      "source": [
        "# This word doesn't 'exist so the word2vec model will error out \r\n",
        "\r\n",
        "# w2v.wv.most_similar(positive = \"scarring\", topn = 5) # uncomment to check"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c61VZl78o5Z6",
        "outputId": "35a7b31d-ae67-4316-b731-1640e942b265"
      },
      "source": [
        "# FastText on the other hand will use n-grams to embed an OOV word\r\n",
        "\r\n",
        "ft.wv.most_similar(positive = \"scarring\", topn = 10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lightning', 0.971960186958313),\n",
              " ('shimmering', 0.9709751605987549),\n",
              " ('prickling', 0.9709533452987671),\n",
              " ('trembling', 0.9704004526138306),\n",
              " ('shivering', 0.9698933959007263),\n",
              " ('clearing', 0.9686659574508667),\n",
              " ('glittering', 0.9681605100631714),\n",
              " ('bearing', 0.9673503637313843),\n",
              " ('fluttering', 0.9671472311019897),\n",
              " ('sparkling', 0.9658246040344238)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEC-9GUXpOug"
      },
      "source": [
        "# Save and load \r\n",
        "\r\n",
        "ft.wv.save(\"model.bin\")\r\n",
        "ft = KeyedVectors.load(\"model.bin\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxT2aaCEskm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}